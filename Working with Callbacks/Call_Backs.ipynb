{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AQDRNrY2NCXf"},"source":["\n","1. Download the data from <a href='https://drive.google.com/file/d/15dCNcmKskcFVjs7R0ElQkR61Ex53uJpM/view?usp=sharing'>here</a>\n","\n","2. Code the model to classify data like below image\n","\n","<img src='https://i.imgur.com/33ptOFy.png'>\n","\n","3. Write your own callback function, that has to print the micro F1 score and AUC score after each epoch.\n","\n","4. Save your model at every epoch if your validation accuracy is improved from previous epoch. \n","\n","5. you have to decay learning based on below conditions \n","        Cond1. If your validation accuracy at that epoch is less than previous epoch accuracy, you have to decrese the\n","               learning rate by 10%. \n","        Cond2. For every 3rd epoch, decay your learning rate by 5%.\n","        \n","6. If you are getting any NaN values(either weigths or loss) while training, you have to terminate your training. \n","\n","7. You have to stop the training if your validation accuracy is not increased in last 2 epochs.\n","\n","8. Use tensorboard for every model and analyse your gradients.\n","\n","9. Use cross entropy as loss function\n","\n","10. Try the architecture params as given below. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"w41Y3TFENCXk"},"source":["<pre>\n","<b>Model-1</b>\n","<pre>\n","1. Use tanh as an activation for every layer except output layer.\n","2. use SGD with momentum as optimizer.\n","3. use RandomUniform(0,1) as initilizer.\n","3. Analyze your output and training process. \n","</pre>\n","</pre>\n","<pre>\n","<b>Model-2</b>\n","<pre>\n","1. Use relu as an activation for every layer except output layer.\n","2. use SGD with momentum as optimizer.\n","3. use RandomUniform(0,1) as initilizer.\n","3. Analyze your output and training process. \n","</pre>\n","</pre>\n","<pre>\n","<b>Model-3</b>\n","<pre>\n","1. Use relu as an activation for every layer except output layer.\n","2. use SGD with momentum as optimizer.\n","3. use he_uniform() as initilizer.\n","3. Analyze your output and training process. \n","</pre>\n","</pre>\n","<pre>\n","<b>Model-4</b>\n","<pre>\n","1. Try with any values to get better accuracy/f1 score.  \n","</pre>\n","</pre>"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","import keras\n","from tensorflow.keras.layers import Dense,Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import TensorBoard"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["df = pd.read_csv('data.csv')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["(20000, 3)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df.shape"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>f1</th>\n","      <th>f2</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.450564</td>\n","      <td>1.074305</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.085632</td>\n","      <td>0.967682</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.117326</td>\n","      <td>0.971521</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.982179</td>\n","      <td>-0.380408</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.720352</td>\n","      <td>0.955850</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         f1        f2  label\n","0  0.450564  1.074305    0.0\n","1  0.085632  0.967682    0.0\n","2  0.117326  0.971521    1.0\n","3  0.982179 -0.380408    0.0\n","4 -0.720352  0.955850    0.0"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["X = df.drop(['label'], axis=1)\n","y = df['label'].values"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["((13400, 2), (6600, 2), (13400,), (6600,))"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["((10720, 2), (2680, 2), (10720,), (2680,))"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","X_train.shape, X_val.shape, y_train.shape, y_val.shape"]},{"cell_type":"markdown","metadata":{},"source":["#### Callback to get micro F1 score and AUC score after each epoch"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["from sklearn.metrics import  f1_score, roc_auc_score\n","class PrintMicroF1andAUCCallback(tf.keras.callbacks.Callback):\n","\n","    def __init__(self, validation_data):\n","        super().__init__()\n","        self.micro_f1_score = tf.keras.metrics.F1Score(name=\"micro_f1_score\")\n","        self.auc_score = tf.keras.metrics.AUC(name='auc_score')\n","        self.validation_data = validation_data\n","\n","    def on_epoch_end(self, epoch, logs):\n","        x_val, y_val = self.validation_data\n","        y_pred = (np.asarray(self.model.predict(x_val))).round()        \n","        \n","        micro_f1_score = f1_score(y_val, y_pred, average='micro')\n","        auc_score = roc_auc_score(y_val, y_pred)\n","        \n","        print(f\"Epoch: {epoch}, Micro F1 Score: {micro_f1_score}, AUC Score: {auc_score}\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Learning rate decay callback"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[],"source":["class CustomLearningRateScheduler(keras.callbacks.Callback):\n","    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n","\n","    Arguments:\n","        schedule: a function that takes an epoch index\n","            (integer, indexed from 0) and current learning rate\n","            as inputs and returns a new learning rate as output (float).\n","    \"\"\"\n","\n","    def __init__(self, schedule):\n","        super().__init__()\n","        self.schedule = schedule\n","\n","    def on_epoch_begin(self, epoch, logs=None):\n","        if not hasattr(self.model.optimizer, \"lr\"):\n","            raise ValueError('Optimizer must have a \"lr\" attribute.')\n","        # Get the current learning rate from model's optimizer.\n","        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n","        # Call schedule function to get the scheduled learning rate.\n","        scheduled_lr = self.schedule(epoch, lr)\n","        # Set the value back to the optimizer before this epoch starts\n","        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n","        print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, scheduled_lr))\n","\n","def lr_schedule(epoch, lr):\n","    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n","    if epoch > 0 and epoch%3 == 0:\n","        return lr - (lr*5/100)\n","    return lr"]},{"cell_type":"markdown","metadata":{},"source":["#### Callback to Terminate on NaN values in Weights or Loss"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[],"source":["class TerminateOnNaNCallback(tf.keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        for weight in self.model.weights:\n","            if tf.reduce_any(tf.math.is_nan(weight)):\n","                print(f\"Epoch {epoch + 1}: NaN values detected in model weights. Training terminated.\")\n","                self.model.stop_training = True\n","        \n","        if tf.math.is_nan(logs['loss']):\n","            print(f\"Epoch {epoch + 1}: NaN values detected in loss. Training terminated.\")\n","            self.model.stop_training = True"]},{"cell_type":"markdown","metadata":{},"source":["#### Callback to Terminate training on no improvement in Val accuracy"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[],"source":["class EarlyStoppingOnValidationAccuracy(tf.keras.callbacks.Callback):\n","    def __init__(self, patience=2):\n","        super().__init__()\n","        self.patience = patience\n","        self.wait = 0\n","        self.best_val_acc = float('-inf')\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        val_acc = logs.get('val_accuracy')\n","        \n","        if val_acc is None:\n","            raise ValueError(\"Validation accuracy is not found in logs. Make sure you are using 'val_accuracy' as the validation metric.\")\n","        \n","        if val_acc > self.best_val_acc:\n","            self.best_val_acc = val_acc\n","            self.wait = 0\n","        else:\n","            self.wait += 1\n","            if self.wait >= self.patience:\n","                print(f\"Epoch {epoch + 1}: Validation accuracy has not improved for {self.patience} consecutive epochs. Training terminated.\")\n","                self.model.stop_training = True"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["# Define the Keras model to add callbacks to\n","def get_model(activation, initializer):\n","    #Input layer\n","    input_layer = Input(shape=(2,))\n","\n","    #Dense hidden layer\n","    layer1 = Dense(50,activation=activation,kernel_initializer=initializer)(input_layer)\n","    layer2 = Dense(40,activation=activation,kernel_initializer=initializer)(layer1)\n","    layer3 = Dense(30,activation=activation,kernel_initializer=initializer)(layer2)\n","    layer4 = Dense(20,activation=activation,kernel_initializer=initializer)(layer3)\n","    layer5 = Dense(10,activation=activation,kernel_initializer=initializer)(layer4)\n","\n","    #output layer\n","    output = Dense(1,activation='sigmoid',kernel_initializer=tf.keras.initializers.RandomUniform(0, 1))(layer5)\n","\n","    #Creating a model\n","    model = Model(inputs=input_layer,outputs=output)\n","    return model"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[],"source":["\n","#Callbacks\n","f1_auc_callback = PrintMicroF1andAUCCallback(\n","                    validation_data=(X_val, y_val))\n","checkpoint_filepath = \"model_save\"\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","                        filepath=checkpoint_filepath,\n","                        save_weights_only=True,\n","                        monitor='val_accuracy',\n","                        mode='max',\n","                        save_best_only=True)\n","reduce_lr_callback = CustomLearningRateScheduler(lr_schedule)\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n","                monitor='val_accuracy', \n","                factor=0.1,\n","                patience=1, \n","                min_lr=1e-7)\n","terminate_nan_callback = TerminateOnNaNCallback()\n","val_acc_terminate_callback = EarlyStoppingOnValidationAccuracy(patience=2)\n","\n","log_dir = \"logs/gradient\"  # Specify the log directory\n","tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)\n","\n","callbacks = [f1_auc_callback, \n","                checkpoint_callback,\n","                reduce_lr_callback,\n","                reduce_lr,\n","                terminate_nan_callback,\n","                val_acc_terminate_callback,\n","                tensorboard_callback]"]},{"cell_type":"markdown","metadata":{},"source":["### Model 1"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Epoch 00000: Learning rate is 0.1000.\n","Epoch 1/10\n","  1/670 [..............................] - ETA: 14:39 - loss: 2.4381 - accuracy: 0.4375WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.1381s). Check your callbacks.\n","84/84 [==============================] - 0s 984us/steposs: 0.7373 - accuracy: 0.\n","Epoch: 0, Micro F1 Score: 0.5406716417910448, AUC Score: 0.5339568741292128\n","670/670 [==============================] - 3s 3ms/step - loss: 0.7368 - accuracy: 0.5206 - val_loss: 0.7923 - val_accuracy: 0.5407 - lr: 0.1000\n","\n","Epoch 00001: Learning rate is 0.1000.\n","Epoch 2/10\n","84/84 [==============================] - 0s 959us/steposs: 0.7360 - accuracy: 0.\n","Epoch: 1, Micro F1 Score: 0.5406716417910448, AUC Score: 0.5339568741292128\n","670/670 [==============================] - 2s 2ms/step - loss: 0.7417 - accuracy: 0.5098 - val_loss: 0.7268 - val_accuracy: 0.5407 - lr: 0.1000\n","\n","Epoch 00002: Learning rate is 0.0100.\n","Epoch 3/10\n","84/84 [==============================] - 0s 1ms/step loss: 0.6875 - accuracy: 0.\n","Epoch: 2, Micro F1 Score: 0.5406716417910448, AUC Score: 0.5339568741292128\n","Epoch 3: Validation accuracy has not improved for 2 consecutive epochs. Training terminated.\n","670/670 [==============================] - 2s 3ms/step - loss: 0.6874 - accuracy: 0.5376 - val_loss: 0.6884 - val_accuracy: 0.5407 - lr: 0.0100\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x25b06526ca0>"]},"execution_count":113,"metadata":{},"output_type":"execute_result"}],"source":["optimizer = tf.keras.optimizers.SGD(0.1, 0.9)\n","initializer = tf.keras.initializers.RandomUniform(0, 1, seed=100)\n","model = get_model('tanh', initializer)\n","# model.summary()\n","\n","model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=16, callbacks=callbacks)"]},{"cell_type":"markdown","metadata":{},"source":["### Model 2"]},{"cell_type":"code","execution_count":116,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Epoch 00000: Learning rate is 0.1000.\n","Epoch 1/10\n","  1/670 [..............................] - ETA: 5:50 - loss: 70868.8516 - accuracy: 0.3750WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.0062s). Check your callbacks.\n","84/84 [==============================] - 0s 1ms/step loss: 109.0597 - accuracy: \n","Epoch: 0, Micro F1 Score: 0.4914179104477612, AUC Score: 0.5\n","Epoch 1: Validation accuracy has not improved for 2 consecutive epochs. Training terminated.\n","670/670 [==============================] - 3s 3ms/step - loss: 106.4719 - accuracy: 0.4983 - val_loss: 0.6933 - val_accuracy: 0.4914 - lr: 0.1000\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x25b08c21760>"]},"execution_count":116,"metadata":{},"output_type":"execute_result"}],"source":["optimizer = tf.keras.optimizers.SGD(0.1, 0.9)\n","initializer = tf.keras.initializers.RandomUniform(0, 1, seed=50)\n","model = get_model('relu', initializer)\n","# model.summary()\n","\n","model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=16, callbacks=callbacks)"]},{"cell_type":"markdown","metadata":{},"source":["### Model 3"]},{"cell_type":"code","execution_count":119,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Epoch 00000: Learning rate is 0.1000.\n","Epoch 1/10\n","  1/670 [..............................] - ETA: 6:31 - loss: 1.3611 - accuracy: 0.3125WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.0085s). Check your callbacks.\n","84/84 [==============================] - 0s 965us/steposs: 0.6944 - accuracy: 0.\n","Epoch: 0, Micro F1 Score: 0.5085820895522388, AUC Score: 0.5\n","Epoch 1: Validation accuracy has not improved for 2 consecutive epochs. Training terminated.\n","670/670 [==============================] - 3s 3ms/step - loss: 0.6944 - accuracy: 0.5183 - val_loss: 0.7126 - val_accuracy: 0.5086 - lr: 0.1000\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x25b75712940>"]},"execution_count":119,"metadata":{},"output_type":"execute_result"}],"source":["optimizer = tf.keras.optimizers.SGD(0.1, 0.9)\n","initializer = tf.keras.initializers.he_uniform(seed=50)\n","model = get_model('relu', initializer)\n","# model.summary()\n","\n","model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=16, callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Call_Backs_Assignment.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}},"nbformat":4,"nbformat_minor":0}
